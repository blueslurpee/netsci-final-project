{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c81eb974",
   "metadata": {},
   "source": [
    "# `1_ingest.ipynb`\n",
    "\n",
    "The following notebook generates analysis dataframes and graphs for the following networks:\n",
    "\n",
    "- Bored Ape Yacht Club\n",
    "- Coolcats\n",
    "- Cryptoadz\n",
    "- Cyberkongz\n",
    "- Hashmasks\n",
    "- Mutant Ape Yacht Club\n",
    "- Meebits\n",
    "- Mekaverse\n",
    "- Sneaky Vampire Syndicate\n",
    "\n",
    "References to these projects and their respective smart contracts can be found in the accompanying report. \n",
    "The notebook first generates **general analysis** dataframes pulling from each project csv file found in the `./data/collated` directory. The notebook persists these dataframes at `./memory/<project>/full.npy` before creating the graph objects from them. \n",
    "\n",
    "Thus, if you want to change the graph models (as we needed to many times over the course of this project), you can do so without re-generating the dataframes themselves. If you only want to generate the graph objects and not the dataframes from scratch, set the `GENERATE_DATAFRAMES` variable in the config section in order to control this behavior.\n",
    "\n",
    "The graphs are then generated from each dataframe as ten successive snapshots of the network. Each snapshot is inclusive of those that came before-hand. In short, the snapshots capture the evolution of the network at different times. Each snapshot is stored at `./memory/<project>/snapshots/` along with a summary dataframe. There is also a `GENERATE_GRAPHS` config variable available as well, which functions analogously to its sibling variable for the dataframes. Note that if both of these variables are set to `False`, the notebook won't produce any output.\n",
    "\n",
    "You can also set the `TEST_LIMIT` config variable, which is helpful for debugging. This limits the anaylsis to the first X rows of the csv files. Note that that upon a successful run the dataframe object will be overwritten, so stash your changes or save a copy in order to restore. \n",
    "\n",
    "The notebook is written in a straightforward and functional style in the attempt to minimize any bloat and be supremely understandable for the reader. Very few of the cells actually run any processes, most are simple function declarations.\n",
    "\n",
    "If you downloaded this project from the github repository, the necessary dataframes and graph snapshots are already generated for you. You can then run them in the `2_analysis.ipynb` notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e931ed06",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093a1fc5",
   "metadata": {},
   "source": [
    "The notebook has one network dependency in the Coinbase API. Set the below environment variables in order to re-run the notebook successfully. This API is used for the ETH-USD conversion rate. Although we generated persistent representations of this data, we used this API for filling in any gaps. Note this is not required if all you want to do is generate the graphs. In that case, simply comment out this portion and set `GENERATE_DATAFRAMES` to `False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60de1a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "\n",
    "import arrow\n",
    "from tqdm import tqdm\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from coinbase.wallet.client import Client\n",
    "\n",
    "load_dotenv('.env')\n",
    "client = Client(os.environ['COINBASE_KEY'], os.environ['COINBASE_SECRET'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbcb2719",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf662372",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_LIMIT = None  # Set to None for production run. Helpful for testing as limits the rows of the dataframe generated\n",
    "GENERATE_DATAFRAMES = False  # Can be set to True to re-generate dataframes from scratch\n",
    "GENERATE_GRAPHS = False  # Can be set to True to re-generate graph objects from scratch\n",
    "\n",
    "projects = [\n",
    "    'bayc',\n",
    "    'coolcats',\n",
    "    'cryptoadz',\n",
    "    'cyberkongz',\n",
    "    'hashmasks',\n",
    "    'mayc',\n",
    "    'meebits',\n",
    "    'mekaverse',\n",
    "    'svs'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b138af",
   "metadata": {},
   "source": [
    "# Generate Analysis Dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b31ad4",
   "metadata": {},
   "source": [
    "### Generate base dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39b33210",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_base_data(project):\n",
    "    PATH_TO_DATA = './data/collated/' + project + '.csv'  # Change if needed\n",
    "    column_names = [\"row\", \"tx_hash\", \"token_address\", \"from_address\", \"to_address\", \"token_id\", \"blk_number\", \"blk_timestamp\", \"eth_value\"]\n",
    "    \n",
    "    df = pd.read_csv(PATH_TO_DATA, delimiter=',', skiprows=1, names=column_names)\n",
    "    \n",
    "    df[\"from_address\"] = df.from_address.apply(lambda x: x.strip())\n",
    "    df[\"to_address\"] = df.to_address.apply(lambda x: x.strip())\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ae960b",
   "metadata": {},
   "source": [
    "### Lookup account data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e24dc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transaction_data(project):\n",
    "    PATH_TO_DATA = f\"./data/balances/{project}.csv\"\n",
    "    return pd.read_csv(PATH_TO_DATA)\n",
    "\n",
    "errors = []\n",
    "\n",
    "def lookup_account_value(df, block, account):\n",
    "    value = 0\n",
    "    df = df.infer_objects()\n",
    "    \n",
    "    if account == '0x0000000000000000000000000000000000000000':\n",
    "        return value\n",
    "    \n",
    "    try:\n",
    "        df_blocked = df[(df['block'] == block) & (df['address'] == account)]\n",
    "        value = df_blocked['eth_value'].head(1).iat[0]\n",
    "    except Exception as e:\n",
    "        errors.append((block, account))\n",
    "    return value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cde2182",
   "metadata": {},
   "source": [
    "### Generate ETH/USD lookup and persist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36aaa7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_eth_to_usd_lookup():\n",
    "    \"\"\"The result is what one ETH is worth in USD\"\"\"\n",
    "    column_names = [\"date\", \"eth_to_usd\"]\n",
    "    df_eth_to_usd = pd.DataFrame(columns=column_names)\n",
    "    \n",
    "    for project in projects:\n",
    "        df_transactions = get_transaction_data(project)\n",
    "        \n",
    "        df_transactions['eth_value'] = df_transactions['eth_value'].apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "        df_transactions['usd_value'] = df_transactions['usd_value'].apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "        \n",
    "        df_transactions = df_transactions.astype({\n",
    "            'eth_value': 'float64',\n",
    "            'usd_value': 'float64'\n",
    "        })\n",
    "        \n",
    "        df_transactions = df_transactions[df_transactions['eth_value'] != 0].groupby('date', as_index=False).first()\n",
    "    \n",
    "        for index, row in tqdm(df_transactions.iterrows(), total=df_transactions.shape[0]):\n",
    "            date = row['date']\n",
    "            eth_to_usd = row['usd_value'] / row['eth_value']\n",
    "\n",
    "            df_eth_to_usd = df_eth_to_usd.append({\n",
    "                'date': date,\n",
    "                'eth_to_usd': eth_to_usd,\n",
    "            }, ignore_index=True)\n",
    "        \n",
    "    df_eth_to_usd = df_eth_to_usd.groupby('date', as_index=False).first()\n",
    "    print(df_eth_to_usd)\n",
    "    \n",
    "    np.save(f\"./memory/eth_to_usd.npy\", df_eth_to_usd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8aa9ecfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 214/214 [00:00<00:00, 1079.57it/s]\n",
      "100%|███████████████████████████████████████| 144/144 [00:00<00:00, 1108.78it/s]\n",
      "100%|█████████████████████████████████████████| 84/84 [00:00<00:00, 1066.64it/s]\n",
      "100%|███████████████████████████████████████| 228/228 [00:00<00:00, 1045.52it/s]\n",
      "100%|███████████████████████████████████████| 277/277 [00:00<00:00, 1099.85it/s]\n",
      "100%|█████████████████████████████████████████| 94/94 [00:00<00:00, 1098.39it/s]\n",
      "100%|███████████████████████████████████████| 212/212 [00:00<00:00, 1106.74it/s]\n",
      "100%|█████████████████████████████████████████| 51/51 [00:00<00:00, 1073.41it/s]\n",
      "100%|█████████████████████████████████████████| 85/85 [00:00<00:00, 1084.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           date  eth_to_usd\n",
      "0    2021-01-28     1240.62\n",
      "1    2021-01-29     1333.61\n",
      "2    2021-01-30     1380.04\n",
      "3    2021-01-31     1380.00\n",
      "4    2021-02-01     1313.95\n",
      "..          ...         ...\n",
      "304  2021-11-28     4098.53\n",
      "305  2021-11-29     4298.38\n",
      "306  2021-11-30     4449.42\n",
      "307  2021-12-01     4636.43\n",
      "308  2021-12-02     4586.87\n",
      "\n",
      "[309 rows x 2 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "build_eth_to_usd_lookup()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb971bca",
   "metadata": {},
   "source": [
    "### Helper functions to get eth_to_usd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7bb463f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "np_data = np.load('./memory/eth_to_usd.npy', allow_pickle=True)\n",
    "df_eth_to_usd = pd.DataFrame(data=np_data, columns=['date', 'eth_to_usd'])\n",
    "\n",
    "def get_eth_to_usd(date):\n",
    "    # This is when you miss static types.. \n",
    "    date = date.strftime(\"%Y-%m-%d\")\n",
    "    rate = df_eth_to_usd.loc[df_eth_to_usd['date'] == date].eth_to_usd.values[0]\n",
    "    return rate\n",
    "\n",
    "# Convert ETH value to USD at specified date\n",
    "def get_usd_value(date, eth_value):\n",
    "    if eth_value == 0:\n",
    "        return eth_value\n",
    "    try:\n",
    "        rate = get_eth_to_usd(date)\n",
    "        return rate * eth_value\n",
    "    except IndexError:\n",
    "        print(\"Date not in values: \" + str(date))\n",
    "        return float(client.get_spot_price(currency_pair='ETH-USD', date=date)['amount']) * eth_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b5f847",
   "metadata": {},
   "source": [
    "### Build time-based analysis dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e0ba4159",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_timed_data(df, df_transactions):\n",
    "    ZERO_ADDRESS = '0x0000000000000000000000000000000000000000'\n",
    "    column_names = [\n",
    "        \"date\", \n",
    "        \"days_since_mint\", \n",
    "        \"from_address\", \n",
    "        \"to_address\", \n",
    "        \"token_id\", \n",
    "        \"blk_number\", \n",
    "        \"eth_value\",\n",
    "        \"usd_value\",\n",
    "        \"from_value\",\n",
    "        \"to_value\",\n",
    "        \"from_value_usd\",\n",
    "        \"to_value_usd\"\n",
    "    ]\n",
    "    \n",
    "    df_time = pd.DataFrame(columns=column_names)\n",
    "    df_total = df.shape[0]\n",
    "    \n",
    "    if TEST_LIMIT:\n",
    "        df = df.head(TEST_LIMIT)\n",
    "        \n",
    "    mint_date_set = False\n",
    "    \n",
    "    for index, row in tqdm(df.iterrows(), total=df_total):\n",
    "        blk_timestamp = row['blk_timestamp']\n",
    "        date = arrow.get(blk_timestamp).datetime\n",
    "\n",
    "        from_address = str(row['from_address'])\n",
    "        to_address = str(row['to_address'])\n",
    "        token_id = row['token_id']\n",
    "        blk_number = row['blk_number']\n",
    "        eth_value = row['eth_value']\n",
    "        usd_value = get_usd_value(date, eth_value)\n",
    "        \n",
    "        if not mint_date_set:\n",
    "            days_since_mint = 0\n",
    "            mint_date = date\n",
    "            mint_date_set = True\n",
    "        else:\n",
    "            days_since_mint = (date - mint_date).days\n",
    "            \n",
    "        from_value = lookup_account_value(df_transactions, blk_number, from_address)\n",
    "        to_value = lookup_account_value(df_transactions, blk_number, to_address)\n",
    "        \n",
    "        from_value_usd = get_usd_value(date, from_value)\n",
    "        to_value_usd = get_usd_value(date, to_value)\n",
    "            \n",
    "        df_time = df_time.append({\n",
    "            'date': date,\n",
    "            'days_since_mint': days_since_mint,\n",
    "            'from_address': from_address,\n",
    "            'to_address': to_address,\n",
    "            'token_id': token_id, \n",
    "            'blk_number': blk_number,\n",
    "            'eth_value': eth_value,\n",
    "            'usd_value': usd_value,\n",
    "            'from_value': from_value,\n",
    "            'to_value': to_value,\n",
    "            'from_value_usd': from_value_usd,\n",
    "            'to_value_usd': to_value_usd,\n",
    "        }, ignore_index=True)\n",
    "    \n",
    "    df_time = df_time.infer_objects()\n",
    "    return df_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c97823b",
   "metadata": {},
   "source": [
    "### Driver code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d94a32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if GENERATE_DATAFRAMES:\n",
    "    for project in projects:\n",
    "        df_transactions = get_transaction_data(project)\n",
    "        df_time = create_timed_data(create_base_data(project), df_transactions)\n",
    "    \n",
    "        np.save(f\"./memory/{project}/full.npy\", df_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05b9b22",
   "metadata": {},
   "source": [
    "# Generate Graph Snapshots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c5ece0",
   "metadata": {},
   "source": [
    "### Build graph objects from time base dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "04a3c719",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_graph_from_timed(df_time, old_graph=None):    \n",
    "    # Building a network per block\n",
    "    # we will use a weighted and directed graph.\n",
    "    graph = old_graph if old_graph is not None else nx.MultiDiGraph()\n",
    "\n",
    "    # loop over the pandas dataframe.\n",
    "    for index, row in tqdm(df_time.iterrows(), total=df_time.shape[0]):\n",
    "        # read the values from the dataframe.\n",
    "        # token_id  blk_timestamp eth_value \n",
    "        date = row['date']\n",
    "        from_address = row['from_address']\n",
    "        to_address = row['to_address']\n",
    "        token_id = row['token_id']\n",
    "        blk_number = row['blk_number']\n",
    "        eth_value = row['eth_value']\n",
    "        usd_value = row['usd_value']\n",
    "        from_value = row['from_value']\n",
    "        to_value = row['to_value']\n",
    "        from_value_usd = row['from_value_usd']\n",
    "        to_value_usd = row['to_value_usd']\n",
    "        \n",
    "        # make sure both addresses are in the graph.\n",
    "        if from_address not in graph:\n",
    "            graph.add_node(from_address)\n",
    "        if to_address not in graph:\n",
    "            graph.add_node(to_address)\n",
    "\n",
    "        # set the attributes on this node.\n",
    "        nx.set_node_attributes(graph, {from_address: from_value, to_address: to_value}, 'eth_value')\n",
    "        nx.set_node_attributes(graph, {from_address: from_value_usd, to_address: to_value_usd}, 'usd_value')\n",
    "\n",
    "        # keep track of how many trades a wallet has done.\n",
    "        trades = nx.get_node_attributes(graph, \"trades\")\n",
    "        if from_address in trades:\n",
    "            nx.set_node_attributes(graph, {from_address:trades[from_address] + 1}, 'trades')\n",
    "        else:\n",
    "            nx.set_node_attributes(graph, {from_address:1}, 'trades')\n",
    "        if to_address in trades:\n",
    "            nx.set_node_attributes(graph, {to_address:trades[to_address] + 1}, 'trades')\n",
    "        else:\n",
    "            nx.set_node_attributes(graph, {to_address:1}, 'trades')\n",
    "\n",
    "        # add an edge for the transaction. # Note changed to usd_value\n",
    "        graph.add_edge(from_address, to_address, weight=usd_value, token_id=token_id) # keep track of token id by adding it to the edge.\n",
    "        \n",
    "    return graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41bb9375",
   "metadata": {},
   "source": [
    "### Build time-based snapshots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "38d51e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_snapshots(df_time):\n",
    "    res = []\n",
    "    column_names = [\n",
    "        \"time_bucket\", \n",
    "        \"time_bucket_label\",\n",
    "        \"number_of_nodes\",\n",
    "        \"degree\",\n",
    "        \"density\",\n",
    "        \"reciprocity\", \n",
    "        \"assortativity\", \n",
    "        \"assortativity_base\", \n",
    "        \"assortativity_out_out\", \n",
    "        \"assortativity_in_in\", \n",
    "        \"assortativity_in_out\",\n",
    "        \"centrality_degree\",\n",
    "        \"centrality_closeness\", \n",
    "    ]\n",
    "    \n",
    "    df_snapshots = pd.DataFrame(columns=column_names)\n",
    "    \n",
    "    df_time['date_quantile'], bins = pd.qcut(df_time['date'], 10, labels=False, retbins=True)\n",
    "    time_buckets = np.unique(df_time[\"date_quantile\"].to_numpy())\n",
    "    \n",
    "    for i, (time_bucket, label) in enumerate(zip(time_buckets, bins)):\n",
    "        graph_selection = df_time[(df_time['date_quantile'] == time_bucket)]\n",
    "        \n",
    "        if i != 0:\n",
    "            old_graph = res[i-1]\n",
    "        else:\n",
    "            old_graph = None\n",
    "        \n",
    "        graph_snapshot = build_graph_from_timed(graph_selection, old_graph=old_graph)\n",
    "        degree = [(node, val) for (node, val) in graph_snapshot.degree()]  # This is necesssary because .degree() returns a *VIEW*\n",
    "        \n",
    "        res.append(graph_snapshot)\n",
    "        df_snapshots = df_snapshots.append({\n",
    "            \"time_bucket\": time_bucket,\n",
    "            \"time_bucket_label\": label,\n",
    "            \"number_of_nodes\": graph_snapshot.number_of_nodes(),\n",
    "            \"degree\": degree,\n",
    "            \"density\": nx.density(graph_snapshot),\n",
    "            \"reciprocity\": nx.reciprocity(graph_snapshot),\n",
    "            \"assortativity\": nx.degree_assortativity_coefficient(graph_snapshot),\n",
    "            \"assortativity_base\": nx.degree_pearson_correlation_coefficient(graph_snapshot.to_undirected(), weight='weight'),\n",
    "            \"assortativity_out_out\": nx.degree_pearson_correlation_coefficient(graph_snapshot, x='out', y='out', weight='weight'),\n",
    "            \"assortativity_in_in\": nx.degree_pearson_correlation_coefficient(graph_snapshot, x='in', y='in', weight='weight'),\n",
    "            \"assortativity_in_out\": nx.degree_pearson_correlation_coefficient(graph_snapshot, x='in', y='out', weight='weight'),\n",
    "            \"centrality_degree\": nx.degree_centrality(graph_snapshot),\n",
    "            \"centrality_closeness\": nx.closeness_centrality(graph_snapshot),\n",
    "        }, ignore_index=True)\n",
    "        \n",
    "    return (df_snapshots.sort_values(by=['time_bucket']), res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3750b089",
   "metadata": {},
   "source": [
    "### Driver code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "10156e77",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if GENERATE_GRAPHS:\n",
    "    for project in projects:\n",
    "        column_names = [\n",
    "            \"date\", \n",
    "            \"days_since_mint\", \n",
    "            \"from_address\", \n",
    "            \"to_address\", \n",
    "            \"token_id\", \n",
    "            \"blk_number\", \n",
    "            \"eth_value\",\n",
    "            \"usd_value\",\n",
    "            \"from_value\", \n",
    "            \"to_value\",\n",
    "            \"from_value_usd\",\n",
    "            \"to_value_usd\"\n",
    "        ]\n",
    "\n",
    "        np_data = np.load(f\"./memory/{project}/full.npy\", allow_pickle=True)\n",
    "        df_time = pd.DataFrame(data=np_data, columns=column_names)\n",
    "\n",
    "        df_snapshot_summary, g_snapshots = build_snapshots(df_time)\n",
    "\n",
    "        for i, snapshot in enumerate(g_snapshots):\n",
    "            nx.write_gml(snapshot, f\"./memory/{project}/snapshots/{i}.gml\")\n",
    "            print(\"Successfully wrote snapshot\")\n",
    "\n",
    "        np.save(f\"./memory/{project}/snapshots/summary.npy\", df_snapshot_summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
