{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "60de1a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "\n",
    "import arrow\n",
    "from tqdm import tqdm\n",
    "\n",
    "# from dotenv import load_dotenv\n",
    "# from coinbase.wallet.client import Client\n",
    "\n",
    "# load_dotenv('.env')\n",
    "# client = Client(os.environ['COINBASE_KEY'], os.environ['COINBASE_SECRET'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd522dc",
   "metadata": {},
   "source": [
    "### Change here to select project output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c26eda0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# projects = ['bayc', 'coolcats', 'cryptoadz', 'cyberkongz', 'hashmasks', 'mayc', 'meebits', 'mekaverse', 'svs']\n",
    "projects = ['mekaverse']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b31ad4",
   "metadata": {},
   "source": [
    "### Store base data as a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "39b33210",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_base_data(project):\n",
    "    PATH_TO_DATA = './data/collated/' + project + '.csv'  # Change if needed\n",
    "    column_names = [\"row\", \"tx_hash\", \"token_address\", \"from_address\", \"to_address\", \"token_id\", \"blk_number\", \"blk_timestamp\", \"eth_value\"]\n",
    "    \n",
    "    df = pd.read_csv(PATH_TO_DATA, delimiter=',', skiprows=1, names=column_names)\n",
    "    \n",
    "    df[\"from_address\"] = df.from_address.apply(lambda x: x.strip())\n",
    "    df[\"to_address\"] = df.to_address.apply(lambda x: x.strip())\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ae960b",
   "metadata": {},
   "source": [
    "### Transaction data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4e24dc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transaction_data(project):\n",
    "    PATH_TO_DATA = f\"./data/balances/{project}.csv\"\n",
    "    return pd.read_csv(PATH_TO_DATA)\n",
    "\n",
    "errors = []\n",
    "\n",
    "def lookup_transaction_value(df, block, account):\n",
    "    value = 0\n",
    "    \n",
    "    if account == '0x0000000000000000000000000000000000000000':\n",
    "        return value\n",
    "    \n",
    "    try:\n",
    "        b = df[(df['block'] == block) & (df['address'] == account)]\n",
    "        value = b['eth_value'].head(1).iat[0]\n",
    "    except Exception as e:\n",
    "        errors.append((block, account))\n",
    "    return value\n",
    "\n",
    "# Convert ETH value to USD at specified date\n",
    "prices = {}\n",
    "def get_usd_value(date, eth_value):\n",
    "    if eth_value == 0:\n",
    "        return eth_value\n",
    "\n",
    "    if date in prices:\n",
    "        return float(prices[date]) * eth_value\n",
    "    else:\n",
    "        prices[date] = client.get_spot_price(currency_pair='ETH-USD', date=date)['amount']\n",
    "        return float(prices[date]) * eth_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b5f847",
   "metadata": {},
   "source": [
    "### Build time-based dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e0ba4159",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_timed_data(df, df_transactions):\n",
    "    ZERO_ADDRESS = '0x0000000000000000000000000000000000000000'\n",
    "    column_names = [\n",
    "        \"date\", \n",
    "        \"days_since_mint\", \n",
    "        \"from_address\", \n",
    "        \"to_address\", \n",
    "        \"token_id\", \n",
    "        \"blk_number\", \n",
    "        \"eth_value\",\n",
    "        #\"usd_value\",\n",
    "        \"from_value\", \n",
    "        \"to_value\",\n",
    "    ]\n",
    "    \n",
    "    df_time = pd.DataFrame(columns=column_names)\n",
    "    df_total = df.shape[0]\n",
    "    \n",
    "    for index, row in tqdm(df.iterrows(), total=df_total):\n",
    "        blk_timestamp = row['blk_timestamp']\n",
    "        date = arrow.get(blk_timestamp).datetime\n",
    "\n",
    "        from_address = row['from_address']\n",
    "        to_address = row['to_address']\n",
    "        token_id = row['token_id']\n",
    "        blk_number = row['blk_number']\n",
    "        eth_value = row['eth_value']\n",
    "        \n",
    "        if from_address == ZERO_ADDRESS:\n",
    "            days_since_mint = 0\n",
    "            mint_date = date\n",
    "        else:\n",
    "            days_since_mint = date - mint_date\n",
    "            \n",
    "        #usd_value = get_usd_value(date, eth_value)\n",
    "            \n",
    "        from_value = lookup_transaction_value(df_transactions, blk_number, from_address)\n",
    "        to_value = lookup_transaction_value(df_transactions, blk_number, to_address)\n",
    "            \n",
    "        df_time = df_time.append({\n",
    "            'date': date,\n",
    "            'days_since_mint': days_since_mint,\n",
    "            'from_address': from_address,\n",
    "            'to_address': to_address,\n",
    "            'token_id': token_id, \n",
    "            'blk_number': blk_number,\n",
    "            'eth_value': eth_value,\n",
    "            #'usd_value': usd_value,\n",
    "            'from_value': from_value,\n",
    "            'to_value': to_value,\n",
    "        }, ignore_index=True)\n",
    "\n",
    "    return df_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c5ece0",
   "metadata": {},
   "source": [
    "### Build graph objects from time base dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "04a3c719",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_graph_from_timed(df_time):    \n",
    "    # Building a network per block\n",
    "    # we will use a weighted and directed graph.\n",
    "    graph = nx.MultiDiGraph()\n",
    "\n",
    "    # loop over the pandas dataframe.\n",
    "    for index, row in tqdm(df_time.iterrows(), total=df_time.shape[0]):\n",
    "        # read the values from the dataframe.\n",
    "        # token_id  blk_timestamp eth_value \n",
    "        date = row['date']\n",
    "        from_address = row['from_address']\n",
    "        to_address = row['to_address']\n",
    "        token_id = row['token_id']\n",
    "        blk_number = row['blk_number']\n",
    "        eth_value = row['eth_value']\n",
    "        #usd_value = row['usd_value']\n",
    "        from_value = row['from_value']\n",
    "        to_value = row['to_value']\n",
    "        \n",
    "        # make sure both addresses are in the graph.\n",
    "        if from_address not in graph:\n",
    "            graph.add_node(from_address)\n",
    "        if to_address not in graph:\n",
    "            graph.add_node(to_address)\n",
    "\n",
    "        # set the attributes on this node.\n",
    "        nx.set_node_attributes(graph, {from_address: from_value, to_address: to_value}, 'eth_value')\n",
    "\n",
    "        # keep track of how many trades a wallet has done.\n",
    "        trades = nx.get_node_attributes(graph, \"trades\")\n",
    "        if from_address in trades:\n",
    "            nx.set_node_attributes(graph, {from_address:trades[from_address] + 1}, 'trades')\n",
    "        else:\n",
    "            nx.set_node_attributes(graph, {from_address:1}, 'trades')\n",
    "        if to_address in trades:\n",
    "            nx.set_node_attributes(graph, {to_address:trades[to_address] + 1}, 'trades')\n",
    "        else:\n",
    "            nx.set_node_attributes(graph, {to_address:1}, 'trades')\n",
    "\n",
    "        # check if this NFT has already been sold and if yes, remove the old sale.\n",
    "        # this might be a candidate for memoization - c.b.\n",
    "        remove_edges = []\n",
    "        for (u,v,d) in graph.edges.data():\n",
    "            if d['token_id'] == token_id:\n",
    "                remove_edges.append((u,v))\n",
    "        # we need to remove them in a seperate step, since otherwise we change the datastructure that we are iterating over.\n",
    "        for (u,v) in remove_edges:\n",
    "            graph.remove_edge(u,v)\n",
    "\n",
    "        # add an edge for the transaction.\n",
    "        graph.add_edge(from_address, to_address, weight=eth_value, token_id=token_id) # keep track of token id by adding it to the edge.\n",
    "        \n",
    "    return graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41bb9375",
   "metadata": {},
   "source": [
    "### Build time-based snapshots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "15e3309b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_graph_snapshots(df_time):\n",
    "    \"\"\"Simpler version of below function which just returns the graph objects\"\"\"\n",
    "    res = {}\n",
    "    \n",
    "    df_time['date_quantile'], bins = pd.qcut(df_time['date'], 10, labels=False, retbins=True)\n",
    "    time_buckets = np.unique(df_time[\"date_quantile\"].to_numpy())\n",
    "    \n",
    "    for time_bucket, label in zip(time_buckets, bins):\n",
    "        selection = df_time[(df_time['date_quantile'] == time_bucket)]\n",
    "        graph_snapshot = build_graph_from_timed(selection)\n",
    "        res[label] = graph_snapshot\n",
    "        \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "38d51e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_df_snapshots(df_time):\n",
    "    column_names = [\n",
    "        \"time_bucket\", \n",
    "        \"time_bucket_label\",\n",
    "        \"number_of_nodes\", \n",
    "        \"avg_clustering\", \n",
    "        \"reciprocity\", \n",
    "        \"assortativity\", \n",
    "        \"assortativity_base\", \n",
    "        \"assortativity_out_out\", \n",
    "        \"assortativity_in_in\", \n",
    "        \"assortativity_in_out\",\n",
    "        \"centrality_degree\",\n",
    "        \"centrality_closeness\", \n",
    "        \"centrality_betweenness\",\n",
    "        \"centrality_eigenvector\",\n",
    "        \"avg_clustering_random\",\n",
    "        \"assortativity_random\"\n",
    "    ]\n",
    "    \n",
    "    df_snapshots = pd.DataFrame(columns=column_names)\n",
    "    \n",
    "    df_time['date_quantile'], bins = pd.qcut(df_time['date'], 10, labels=False, retbins=True)\n",
    "    time_buckets = np.unique(df_time[\"date_quantile\"].to_numpy())\n",
    "    \n",
    "    for time_bucket, label in zip(time_buckets, bins):\n",
    "        selection = df_time[(df_time['date_quantile'] == time_bucket)]\n",
    "        graph_snapshot = build_graph_from_timed(selection)\n",
    "        \n",
    "        df_snapshots = df_snapshots.append({\n",
    "            \"time_bucket\": time_bucket,\n",
    "            \"time_bucket_label\": label,\n",
    "            \"number_of_nodes\": graph_snapshot.number_of_nodes(),\n",
    "            \"reciprocity\": nx.reciprocity(graph_snapshot),\n",
    "            \"assortativity\": nx.degree_assortativity_coefficient(graph_snapshot),\n",
    "            \"assortativity_base\": nx.degree_pearson_correlation_coefficient(graph_snapshot.to_undirected(), weight='weight'),\n",
    "            \"assortativity_out_out\": nx.degree_pearson_correlation_coefficient(graph_snapshot, x='out', y='out', weight='weight'),\n",
    "            \"assortativity_in_in\": nx.degree_pearson_correlation_coefficient(graph_snapshot, x='in', y='in', weight='weight'),\n",
    "            \"assortativity_in_out\": nx.degree_pearson_correlation_coefficient(graph_snapshot, x='in', y='out', weight='weight'),\n",
    "            \"centrality_degree\": nx.degree_centrality(graph_snapshot),\n",
    "            \"centrality_closeness\": nx.closeness_centrality(graph_snapshot),\n",
    "        }, ignore_index=True)\n",
    "        \n",
    "    return df_snapshots.sort_values(by=['time_bucket'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3d94a32b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22262/22262 [04:34<00:00, 80.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "522\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22262/22262 [03:08<00:00, 117.85it/s]\n"
     ]
    }
   ],
   "source": [
    "for project in projects:\n",
    "    df_transactions = get_transaction_data(project)\n",
    "    df_time = create_timed_data(create_base_data(project), df_transactions)\n",
    "    print(len(errors))\n",
    "    \n",
    "    g_time = build_graph_from_timed(df_time)\n",
    "    \n",
    "    np.save(f\"./memory/{project}/full.npy\", df_time)\n",
    "    nx.write_gml(g_time, f\"./memory/{project}/full.gml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "10156e77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2302/2302 [00:02<00:00, 1101.13it/s]\n",
      "100%|██████████| 2181/2181 [00:01<00:00, 1143.64it/s]\n",
      "100%|██████████| 2205/2205 [00:02<00:00, 1006.82it/s]\n",
      "100%|██████████| 2235/2235 [00:02<00:00, 927.45it/s] \n",
      "100%|██████████| 2213/2213 [00:02<00:00, 964.64it/s] \n",
      "100%|██████████| 2221/2221 [00:02<00:00, 972.62it/s] \n",
      "100%|██████████| 2226/2226 [00:02<00:00, 762.80it/s] \n",
      "100%|██████████| 2226/2226 [00:02<00:00, 793.12it/s] \n",
      "100%|██████████| 2291/2291 [00:02<00:00, 952.11it/s] \n",
      "100%|██████████| 2162/2162 [00:02<00:00, 1004.34it/s]\n",
      "100%|██████████| 2302/2302 [00:02<00:00, 1069.46it/s]\n",
      "100%|██████████| 2181/2181 [00:01<00:00, 1218.64it/s]\n",
      "100%|██████████| 2205/2205 [00:01<00:00, 1184.35it/s]\n",
      "100%|██████████| 2235/2235 [00:02<00:00, 997.97it/s] \n",
      "100%|██████████| 2213/2213 [00:01<00:00, 1148.90it/s]\n",
      "100%|██████████| 2221/2221 [00:02<00:00, 1062.76it/s]\n",
      "100%|██████████| 2226/2226 [00:02<00:00, 748.15it/s] \n",
      "100%|██████████| 2226/2226 [00:02<00:00, 899.07it/s] \n",
      "100%|██████████| 2291/2291 [00:02<00:00, 887.93it/s] \n",
      "100%|██████████| 2162/2162 [00:01<00:00, 1096.76it/s]\n"
     ]
    }
   ],
   "source": [
    "for project in projects:\n",
    "    column_names = [\n",
    "        \"date\", \n",
    "        \"days_since_mint\", \n",
    "        \"from_address\", \n",
    "        \"to_address\", \n",
    "        \"token_id\", \n",
    "        \"blk_number\", \n",
    "        \"eth_value\",\n",
    "        \"from_value\", \n",
    "        \"to_value\",\n",
    "    ]\n",
    "    \n",
    "    np_data = np.load(f\"./memory/{project}/full.npy\", allow_pickle=True)\n",
    "    df_time = pd.DataFrame(data=np_data, columns=column_names)\n",
    "    \n",
    "    g_snapshots = build_graph_snapshots(df_time)\n",
    "    df_snapshot_summary = build_df_snapshots(df_time)\n",
    "    \n",
    "    for i, snapshot in enumerate(g_snapshots.keys()):\n",
    "        nx.write_gml(g_snapshots[snapshot], f\"./memory/{project}/snapshots/{i}.gml\")\n",
    "    \n",
    "    np.save(f\"./memory/{project}/snapshots/summary.npy\", df_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e7715d0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         row     block        date  \\\n",
      "0          0  12253142  2021-04-16   \n",
      "1          2  12253527  2021-04-16   \n",
      "2          3  12253570  2021-04-16   \n",
      "3          4  12253684  2021-04-16   \n",
      "4          5  12253689  2021-04-16   \n",
      "...      ...       ...         ...   \n",
      "21229  21230  13711959  2021-11-30   \n",
      "21230  21231  13711968  2021-11-30   \n",
      "21231  21232  13711968  2021-11-30   \n",
      "21232  21233  13711969  2021-11-30   \n",
      "21233  21234  13711969  2021-11-30   \n",
      "\n",
      "                                          address  eth_value     usd_value  \n",
      "0      0x000000048797808be86aa9786c1f402671192d6b   1.341069   3375.509898  \n",
      "1      0x721931508df2764fd4f70c53da646cb8aed16ace   1.231443   3099.578481  \n",
      "2      0x721931508df2764fd4f70c53da646cb8aed16ace   1.193960   3005.233539  \n",
      "3      0xfa6c54de608c9a0a2c2a3220bb7e42b95d1b910b   0.473936   1192.912132  \n",
      "4      0xfa6c54de608c9a0a2c2a3220bb7e42b95d1b910b   0.432051   1087.484855  \n",
      "...                                           ...        ...           ...  \n",
      "21229  0xa145d810852499f0a45ae65d8545e74a78e9b92e   7.390825  32884.883251  \n",
      "21230  0xa145d810852499f0a45ae65d8545e74a78e9b92e   7.364025  32765.641147  \n",
      "21231  0xb14b87790643d2dab44b06692d37dd95b4b30e56   0.000000      0.000000  \n",
      "21232  0xb14b87790643d2dab44b06692d37dd95b4b30e56   0.000000      0.000000  \n",
      "21233  0xa145d810852499f0a45ae65d8545e74a78e9b92e   7.337746  32648.715996  \n",
      "\n",
      "[21234 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "print(get_transaction_data('cyberkongz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e6a2602c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}