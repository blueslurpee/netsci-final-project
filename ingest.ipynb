{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "60de1a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "\n",
    "import arrow\n",
    "from tqdm import tqdm\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from coinbase.wallet.client import Client\n",
    "\n",
    "load_dotenv('.env')\n",
    "client = Client(os.environ['COINBASE_KEY'], os.environ['COINBASE_SECRET'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbcb2719",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "bf662372",
   "metadata": {},
   "outputs": [],
   "source": [
    "SETUP_ETH_TO_USD = True  # Should be True for first run, thereafter can be set to False\n",
    "TEST_LIMIT = None # Set to None for production run\n",
    "\n",
    "projects = [\n",
    "#     'bayc',\n",
    "#     'coolcats',\n",
    "#     'cryptoadz',\n",
    "#     'cyberkongz',\n",
    "#     'hashmasks',\n",
    "#     'mayc',\n",
    "#     'meebits',\n",
    "#     'mekaverse',\n",
    "#     'svs'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b31ad4",
   "metadata": {},
   "source": [
    "### Store base data as a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "39b33210",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_base_data(project):\n",
    "    PATH_TO_DATA = './data/collated/' + project + '.csv'  # Change if needed\n",
    "    column_names = [\"row\", \"tx_hash\", \"token_address\", \"from_address\", \"to_address\", \"token_id\", \"blk_number\", \"blk_timestamp\", \"eth_value\"]\n",
    "    \n",
    "    df = pd.read_csv(PATH_TO_DATA, delimiter=',', skiprows=1, names=column_names)\n",
    "    \n",
    "    df[\"from_address\"] = df.from_address.apply(lambda x: x.strip())\n",
    "    df[\"to_address\"] = df.to_address.apply(lambda x: x.strip())\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ae960b",
   "metadata": {},
   "source": [
    "### Transaction data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "4e24dc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transaction_data(project):\n",
    "    PATH_TO_DATA = f\"./data/balances/{project}.csv\"\n",
    "    return pd.read_csv(PATH_TO_DATA)\n",
    "\n",
    "errors = []\n",
    "\n",
    "def lookup_account_value(df, block, account):\n",
    "    value = 0\n",
    "    df = df.infer_objects()\n",
    "    \n",
    "    if account == '0x0000000000000000000000000000000000000000':\n",
    "        return value\n",
    "    \n",
    "    try:\n",
    "        df_blocked = df[(df['block'] == block) & (df['address'] == account)]\n",
    "        value = df_blocked['eth_value'].head(1).iat[0]\n",
    "    except Exception as e:\n",
    "        errors.append((block, account))\n",
    "    return value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cde2182",
   "metadata": {},
   "source": [
    "### Setup ETH/USD data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "36aaa7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_eth_to_usd_lookup():\n",
    "    \"\"\"The result is what one ETH is worth in USD\"\"\"\n",
    "    column_names = [\"date\", \"eth_to_usd\"]\n",
    "    df_eth_to_usd = pd.DataFrame(columns=column_names)\n",
    "    \n",
    "    for project in projects:\n",
    "        df_transactions = get_transaction_data(project)\n",
    "        \n",
    "        df_transactions['eth_value'] = df_transactions['eth_value'].apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "        df_transactions['usd_value'] = df_transactions['usd_value'].apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "        \n",
    "        df_transactions = df_transactions.astype({\n",
    "            'eth_value': 'float64',\n",
    "            'usd_value': 'float64'\n",
    "        })\n",
    "        \n",
    "        df_transactions = df_transactions[df_transactions['eth_value'] != 0].groupby('date', as_index=False).first()\n",
    "    \n",
    "        for index, row in tqdm(df_transactions.iterrows(), total=df_transactions.shape[0]):\n",
    "            date = row['date']\n",
    "            eth_to_usd = row['usd_value'] / row['eth_value']\n",
    "\n",
    "            df_eth_to_usd = df_eth_to_usd.append({\n",
    "                'date': date,\n",
    "                'eth_to_usd': eth_to_usd,\n",
    "            }, ignore_index=True)\n",
    "        \n",
    "    df_eth_to_usd = df_eth_to_usd.groupby('date', as_index=False).first()\n",
    "    print(df_eth_to_usd)\n",
    "    \n",
    "    np.save(f\"./memory/eth_to_usd.npy\", df_eth_to_usd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "8aa9ecfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 277/277 [00:00<00:00, 1166.13it/s]\n",
      "100%|█████████████████████████████████████████| 94/94 [00:00<00:00, 1171.22it/s]\n",
      "100%|███████████████████████████████████████| 212/212 [00:00<00:00, 1142.34it/s]\n",
      "100%|█████████████████████████████████████████| 85/85 [00:00<00:00, 1123.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           date  eth_to_usd\n",
      "0    2021-01-28     1240.62\n",
      "1    2021-01-29     1333.61\n",
      "2    2021-01-30     1380.04\n",
      "3    2021-01-31     1380.00\n",
      "4    2021-02-01     1313.95\n",
      "..          ...         ...\n",
      "304  2021-11-28     4098.53\n",
      "305  2021-11-29     4298.38\n",
      "306  2021-11-30     4449.42\n",
      "307  2021-12-01     4636.43\n",
      "308  2021-12-02     4586.87\n",
      "\n",
      "[309 rows x 2 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if SETUP_ETH_TO_USD:\n",
    "    build_eth_to_usd_lookup()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb971bca",
   "metadata": {},
   "source": [
    "### Helper function to get eth_to_usd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "7bb463f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "np_data = np.load('./memory/eth_to_usd.npy', allow_pickle=True)\n",
    "df_eth_to_usd = pd.DataFrame(data=np_data, columns=['date', 'eth_to_usd'])\n",
    "\n",
    "def get_eth_to_usd(date):\n",
    "    # This is when you miss static types.. \n",
    "    date = date.strftime(\"%Y-%m-%d\")\n",
    "    rate = df_eth_to_usd.loc[df_eth_to_usd['date'] == date].eth_to_usd.values[0]\n",
    "    return rate\n",
    "\n",
    "# Convert ETH value to USD at specified date\n",
    "def get_usd_value(date, eth_value):\n",
    "    if eth_value == 0:\n",
    "        return eth_value\n",
    "    try:\n",
    "        rate = get_eth_to_usd(date)\n",
    "        return rate * eth_value\n",
    "    except IndexError:\n",
    "        print(\"Date not in values: \" + str(date))\n",
    "        return float(client.get_spot_price(currency_pair='ETH-USD', date=date)['amount']) * eth_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b5f847",
   "metadata": {},
   "source": [
    "### Build time-based dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "e0ba4159",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_timed_data(df, df_transactions):\n",
    "    ZERO_ADDRESS = '0x0000000000000000000000000000000000000000'\n",
    "    column_names = [\n",
    "        \"date\", \n",
    "        \"days_since_mint\", \n",
    "        \"from_address\", \n",
    "        \"to_address\", \n",
    "        \"token_id\", \n",
    "        \"blk_number\", \n",
    "        \"eth_value\",\n",
    "        \"usd_value\",\n",
    "        \"from_value\",\n",
    "        \"to_value\",\n",
    "        \"from_value_usd\",\n",
    "        \"to_value_usd\"\n",
    "    ]\n",
    "    \n",
    "    df_time = pd.DataFrame(columns=column_names)\n",
    "    df_total = df.shape[0]\n",
    "    \n",
    "    if TEST_LIMIT:\n",
    "        df = df.head(TEST_LIMIT)\n",
    "        \n",
    "    mint_date_set = False\n",
    "    \n",
    "    for index, row in tqdm(df.iterrows(), total=df_total):\n",
    "        blk_timestamp = row['blk_timestamp']\n",
    "        date = arrow.get(blk_timestamp).datetime\n",
    "\n",
    "        from_address = str(row['from_address'])\n",
    "        to_address = str(row['to_address'])\n",
    "        token_id = row['token_id']\n",
    "        blk_number = row['blk_number']\n",
    "        eth_value = row['eth_value']\n",
    "        usd_value = get_usd_value(date, eth_value)\n",
    "        \n",
    "        if not mint_date_set:\n",
    "            days_since_mint = 0\n",
    "            mint_date = date\n",
    "            mint_date_set = True\n",
    "        else:\n",
    "            days_since_mint = (date - mint_date).days\n",
    "            \n",
    "        from_value = lookup_account_value(df_transactions, blk_number, from_address)\n",
    "        to_value = lookup_account_value(df_transactions, blk_number, to_address)\n",
    "        \n",
    "        from_value_usd = get_usd_value(date, from_value)\n",
    "        to_value_usd = get_usd_value(date, to_value)\n",
    "            \n",
    "        df_time = df_time.append({\n",
    "            'date': date,\n",
    "            'days_since_mint': days_since_mint,\n",
    "            'from_address': from_address,\n",
    "            'to_address': to_address,\n",
    "            'token_id': token_id, \n",
    "            'blk_number': blk_number,\n",
    "            'eth_value': eth_value,\n",
    "            'usd_value': usd_value,\n",
    "            'from_value': from_value,\n",
    "            'to_value': to_value,\n",
    "            'from_value_usd': from_value_usd,\n",
    "            'to_value_usd': to_value_usd,\n",
    "        }, ignore_index=True)\n",
    "    \n",
    "    df_time = df_time.infer_objects()\n",
    "    return df_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c5ece0",
   "metadata": {},
   "source": [
    "### Build graph objects from time base dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "04a3c719",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_graph_from_timed(df_time, old_graph=None):    \n",
    "    # Building a network per block\n",
    "    # we will use a weighted and directed graph.\n",
    "    graph = old_graph if old_graph is not None else nx.MultiDiGraph()\n",
    "\n",
    "    # loop over the pandas dataframe.\n",
    "    for index, row in tqdm(df_time.iterrows(), total=df_time.shape[0]):\n",
    "        # read the values from the dataframe.\n",
    "        # token_id  blk_timestamp eth_value \n",
    "        date = row['date']\n",
    "        from_address = row['from_address']\n",
    "        to_address = row['to_address']\n",
    "        token_id = row['token_id']\n",
    "        blk_number = row['blk_number']\n",
    "        eth_value = row['eth_value']\n",
    "        usd_value = row['usd_value']\n",
    "        from_value = row['from_value']\n",
    "        to_value = row['to_value']\n",
    "        from_value_usd = row['from_value_usd']\n",
    "        to_value_usd = row['to_value_usd']\n",
    "        \n",
    "        # make sure both addresses are in the graph.\n",
    "        if from_address not in graph:\n",
    "            graph.add_node(from_address)\n",
    "        if to_address not in graph:\n",
    "            graph.add_node(to_address)\n",
    "\n",
    "        # set the attributes on this node.\n",
    "        nx.set_node_attributes(graph, {from_address: from_value, to_address: to_value}, 'eth_value')\n",
    "        nx.set_node_attributes(graph, {from_address: from_value_usd, to_address: to_value_usd}, 'usd_value')\n",
    "\n",
    "        # keep track of how many trades a wallet has done.\n",
    "        trades = nx.get_node_attributes(graph, \"trades\")\n",
    "        if from_address in trades:\n",
    "            nx.set_node_attributes(graph, {from_address:trades[from_address] + 1}, 'trades')\n",
    "        else:\n",
    "            nx.set_node_attributes(graph, {from_address:1}, 'trades')\n",
    "        if to_address in trades:\n",
    "            nx.set_node_attributes(graph, {to_address:trades[to_address] + 1}, 'trades')\n",
    "        else:\n",
    "            nx.set_node_attributes(graph, {to_address:1}, 'trades')\n",
    "\n",
    "        # check if this NFT has already been sold and if yes, remove the old sale.\n",
    "        # this might be a candidate for memoization - c.b.\n",
    "        remove_edges = []\n",
    "        for (u,v,d) in graph.edges.data():\n",
    "            if d['token_id'] == token_id:\n",
    "                remove_edges.append((u,v))\n",
    "        # we need to remove them in a seperate step, since otherwise we change the datastructure that we are iterating over.\n",
    "        for (u,v) in remove_edges:\n",
    "            graph.remove_edge(u,v)\n",
    "\n",
    "        # add an edge for the transaction. # Note changed to usd_value\n",
    "        graph.add_edge(from_address, to_address, weight=usd_value, token_id=token_id) # keep track of token id by adding it to the edge.\n",
    "        \n",
    "    return graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41bb9375",
   "metadata": {},
   "source": [
    "### Build time-based snapshots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "38d51e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_snapshots(df_time):\n",
    "    res = []\n",
    "    column_names = [\n",
    "        \"time_bucket\", \n",
    "        \"time_bucket_label\",\n",
    "        \"number_of_nodes\", \n",
    "        \"reciprocity\", \n",
    "        \"assortativity\", \n",
    "        \"assortativity_base\", \n",
    "        \"assortativity_out_out\", \n",
    "        \"assortativity_in_in\", \n",
    "        \"assortativity_in_out\",\n",
    "        \"centrality_degree\",\n",
    "        \"centrality_closeness\", \n",
    "    ]\n",
    "    \n",
    "    df_snapshots = pd.DataFrame(columns=column_names)\n",
    "    \n",
    "    df_time['date_quantile'], bins = pd.qcut(df_time['date'], 10, labels=False, retbins=True)\n",
    "    time_buckets = np.unique(df_time[\"date_quantile\"].to_numpy())\n",
    "    \n",
    "    for i, (time_bucket, label) in enumerate(zip(time_buckets, bins)):\n",
    "        graph_selection = df_time[(df_time['date_quantile'] == time_bucket)]\n",
    "        \n",
    "        if i != 0:\n",
    "            old_graph = res[i-1]\n",
    "        else:\n",
    "            old_graph = None\n",
    "        \n",
    "        graph_snapshot = build_graph_from_timed(graph_selection, old_graph=old_graph)\n",
    "        \n",
    "        res.append(graph_snapshot)\n",
    "        df_snapshots = df_snapshots.append({\n",
    "            \"time_bucket\": time_bucket,\n",
    "            \"time_bucket_label\": label,\n",
    "            \"number_of_nodes\": graph_snapshot.number_of_nodes(),\n",
    "            \"reciprocity\": nx.reciprocity(graph_snapshot),\n",
    "            \"assortativity\": nx.degree_assortativity_coefficient(graph_snapshot),\n",
    "            \"assortativity_base\": nx.degree_pearson_correlation_coefficient(graph_snapshot.to_undirected(), weight='weight'),\n",
    "            \"assortativity_out_out\": nx.degree_pearson_correlation_coefficient(graph_snapshot, x='out', y='out', weight='weight'),\n",
    "            \"assortativity_in_in\": nx.degree_pearson_correlation_coefficient(graph_snapshot, x='in', y='in', weight='weight'),\n",
    "            \"assortativity_in_out\": nx.degree_pearson_correlation_coefficient(graph_snapshot, x='in', y='out', weight='weight'),\n",
    "            \"centrality_degree\": nx.degree_centrality(graph_snapshot),\n",
    "            \"centrality_closeness\": nx.closeness_centrality(graph_snapshot),\n",
    "        }, ignore_index=True)\n",
    "        \n",
    "    return (df_snapshots.sort_values(by=['time_bucket']), res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "3d94a32b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 60658/60658 [13:00<00:00, 77.70it/s]\n",
      "100%|█████████████████████████████████████| 46497/46497 [09:34<00:00, 80.97it/s]\n",
      "100%|█████████████████████████████████████| 43112/43112 [07:27<00:00, 96.37it/s]\n",
      "100%|████████████████████████████████████| 34043/34043 [05:34<00:00, 101.80it/s]\n"
     ]
    }
   ],
   "source": [
    "for project in projects:\n",
    "    df_transactions = get_transaction_data(project)\n",
    "    df_time = create_timed_data(create_base_data(project), df_transactions)\n",
    "    \n",
    "    np.save(f\"./memory/{project}/full.npy\", df_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "10156e77",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 6066/6066 [00:04<00:00, 1299.26it/s]\n",
      "100%|██████████████████████████████████████| 6066/6066 [00:14<00:00, 416.78it/s]\n",
      "100%|██████████████████████████████████████| 6066/6066 [00:27<00:00, 222.66it/s]\n",
      "100%|██████████████████████████████████████| 6065/6065 [00:36<00:00, 166.29it/s]\n",
      "100%|██████████████████████████████████████| 6066/6066 [00:43<00:00, 140.60it/s]\n",
      "100%|██████████████████████████████████████| 6066/6066 [00:48<00:00, 123.82it/s]\n",
      "100%|██████████████████████████████████████| 6067/6067 [00:54<00:00, 111.04it/s]\n",
      "100%|██████████████████████████████████████| 6064/6064 [00:58<00:00, 103.78it/s]\n",
      "100%|███████████████████████████████████████| 6066/6066 [01:02<00:00, 97.73it/s]\n",
      "100%|███████████████████████████████████████| 6066/6066 [01:10<00:00, 85.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully wrote snapshot\n",
      "Successfully wrote snapshot\n",
      "Successfully wrote snapshot\n",
      "Successfully wrote snapshot\n",
      "Successfully wrote snapshot\n",
      "Successfully wrote snapshot\n",
      "Successfully wrote snapshot\n",
      "Successfully wrote snapshot\n",
      "Successfully wrote snapshot\n",
      "Successfully wrote snapshot\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 4658/4658 [00:04<00:00, 997.45it/s]\n",
      "100%|██████████████████████████████████████| 4711/4711 [00:13<00:00, 356.90it/s]\n",
      "100%|██████████████████████████████████████| 4584/4584 [00:21<00:00, 208.44it/s]\n",
      "100%|██████████████████████████████████████| 4646/4646 [00:33<00:00, 138.71it/s]\n",
      "100%|███████████████████████████████████████| 4650/4650 [00:46<00:00, 99.17it/s]\n",
      "100%|███████████████████████████████████████| 4649/4649 [00:58<00:00, 79.25it/s]\n",
      "100%|███████████████████████████████████████| 4650/4650 [01:07<00:00, 68.69it/s]\n",
      "100%|███████████████████████████████████████| 4649/4649 [01:16<00:00, 60.95it/s]\n",
      "100%|███████████████████████████████████████| 4650/4650 [01:26<00:00, 54.07it/s]\n",
      "100%|███████████████████████████████████████| 4650/4650 [01:33<00:00, 49.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully wrote snapshot\n",
      "Successfully wrote snapshot\n",
      "Successfully wrote snapshot\n",
      "Successfully wrote snapshot\n",
      "Successfully wrote snapshot\n",
      "Successfully wrote snapshot\n",
      "Successfully wrote snapshot\n",
      "Successfully wrote snapshot\n",
      "Successfully wrote snapshot\n",
      "Successfully wrote snapshot\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 4317/4317 [00:03<00:00, 1098.03it/s]\n",
      "100%|██████████████████████████████████████| 4313/4313 [00:10<00:00, 403.80it/s]\n",
      "100%|██████████████████████████████████████| 4316/4316 [00:17<00:00, 241.87it/s]\n",
      "100%|██████████████████████████████████████| 4299/4299 [00:26<00:00, 159.95it/s]\n",
      "100%|██████████████████████████████████████| 4311/4311 [00:31<00:00, 136.99it/s]\n",
      "100%|██████████████████████████████████████| 4311/4311 [00:36<00:00, 117.50it/s]\n",
      "100%|██████████████████████████████████████| 4311/4311 [00:42<00:00, 100.79it/s]\n",
      "100%|███████████████████████████████████████| 4315/4315 [00:49<00:00, 87.64it/s]\n",
      "100%|███████████████████████████████████████| 4307/4307 [00:51<00:00, 82.85it/s]\n",
      "100%|███████████████████████████████████████| 4312/4312 [00:57<00:00, 75.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully wrote snapshot\n",
      "Successfully wrote snapshot\n",
      "Successfully wrote snapshot\n",
      "Successfully wrote snapshot\n",
      "Successfully wrote snapshot\n",
      "Successfully wrote snapshot\n",
      "Successfully wrote snapshot\n",
      "Successfully wrote snapshot\n",
      "Successfully wrote snapshot\n",
      "Successfully wrote snapshot\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 3420/3420 [00:02<00:00, 1295.96it/s]\n",
      "100%|██████████████████████████████████████| 3438/3438 [00:07<00:00, 438.75it/s]\n",
      "100%|██████████████████████████████████████| 3362/3362 [00:12<00:00, 262.28it/s]\n",
      "100%|██████████████████████████████████████| 3397/3397 [00:17<00:00, 190.93it/s]\n",
      "100%|██████████████████████████████████████| 3405/3405 [00:21<00:00, 157.12it/s]\n",
      "100%|██████████████████████████████████████| 3404/3404 [00:25<00:00, 133.10it/s]\n",
      "100%|██████████████████████████████████████| 3404/3404 [00:28<00:00, 120.76it/s]\n",
      "100%|██████████████████████████████████████| 3405/3405 [00:28<00:00, 118.99it/s]\n",
      "100%|██████████████████████████████████████| 3403/3403 [00:28<00:00, 119.21it/s]\n",
      "100%|██████████████████████████████████████| 3405/3405 [00:28<00:00, 119.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully wrote snapshot\n",
      "Successfully wrote snapshot\n",
      "Successfully wrote snapshot\n",
      "Successfully wrote snapshot\n",
      "Successfully wrote snapshot\n",
      "Successfully wrote snapshot\n",
      "Successfully wrote snapshot\n",
      "Successfully wrote snapshot\n",
      "Successfully wrote snapshot\n",
      "Successfully wrote snapshot\n"
     ]
    }
   ],
   "source": [
    "for project in projects:\n",
    "    column_names = [\n",
    "        \"date\", \n",
    "        \"days_since_mint\", \n",
    "        \"from_address\", \n",
    "        \"to_address\", \n",
    "        \"token_id\", \n",
    "        \"blk_number\", \n",
    "        \"eth_value\",\n",
    "        \"usd_value\",\n",
    "        \"from_value\", \n",
    "        \"to_value\",\n",
    "        \"from_value_usd\",\n",
    "        \"to_value_usd\"\n",
    "    ]\n",
    "    \n",
    "    np_data = np.load(f\"./memory/{project}/full.npy\", allow_pickle=True)\n",
    "    df_time = pd.DataFrame(data=np_data, columns=column_names)\n",
    "    \n",
    "    df_snapshot_summary, g_snapshots = build_snapshots(df_time)\n",
    "    \n",
    "    for i, snapshot in enumerate(g_snapshots):\n",
    "        nx.write_gml(snapshot, f\"./memory/{project}/snapshots/{i}.gml\")\n",
    "        print(\"Successfully wrote snapshot\")\n",
    "    \n",
    "    np.save(f\"./memory/{project}/snapshots/summary.npy\", df_snapshot_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21534b95",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}