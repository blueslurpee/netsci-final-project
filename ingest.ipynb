{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60de1a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "\n",
    "import arrow\n",
    "from tqdm import tqdm\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from coinbase.wallet.client import Client\n",
    "\n",
    "load_dotenv('.env')\n",
    "client = Client(os.environ['COINBASE_KEY'], os.environ['COINBASE_SECRET'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd522dc",
   "metadata": {},
   "source": [
    "### Change here to select project output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c26eda0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# projects = ['bayc', 'coolcats', 'cryptoadz', 'cyberkongz', 'hashmasks', 'mayc', 'meebits', 'mekaverse', 'svs']\n",
    "projects = ['cryptoadz', 'cyberkongz']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b31ad4",
   "metadata": {},
   "source": [
    "### Store base data as a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39b33210",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_base_data(project):\n",
    "    PATH_TO_DATA = './data/collated/' + project + '.csv'  # Change if needed\n",
    "    column_names = [\"row\", \"tx_hash\", \"token_address\", \"from_address\", \"to_address\", \"token_id\", \"blk_number\", \"blk_timestamp\", \"eth_value\"]\n",
    "    \n",
    "    df = pd.read_csv(PATH_TO_DATA, delimiter=',', skiprows=1, names=column_names)\n",
    "    \n",
    "    df[\"from_address\"] = df.from_address.apply(lambda x: x.strip())\n",
    "    df[\"to_address\"] = df.to_address.apply(lambda x: x.strip())\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ae960b",
   "metadata": {},
   "source": [
    "### Transaction data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e24dc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transaction_data(project):\n",
    "    PATH_TO_DATA = f\"./data/balances/{project}.csv\"\n",
    "    return pd.read_csv(PATH_TO_DATA)\n",
    "\n",
    "errors = []\n",
    "\n",
    "def lookup_transaction_value(df, block, account):\n",
    "    value = 0\n",
    "    \n",
    "    if account == '0x0000000000000000000000000000000000000000':\n",
    "        return value\n",
    "    \n",
    "    try:\n",
    "        b = df[(df['block'] == block) & (df['address'] == account)]\n",
    "        value = b['eth_value'].head(1).iat[0]\n",
    "    except Exception as e:\n",
    "        errors.append((block, account))\n",
    "    return value\n",
    "\n",
    "# Convert ETH value to USD at specified date\n",
    "prices = {}\n",
    "def get_usd_value(date, eth_value):\n",
    "    if eth_value == 0:\n",
    "        return eth_value\n",
    "\n",
    "    if date in prices:\n",
    "        return float(prices[date]) * eth_value\n",
    "    else:\n",
    "        prices[date] = client.get_spot_price(currency_pair='ETH-USD', date=date)['amount']\n",
    "        return float(prices[date]) * eth_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b5f847",
   "metadata": {},
   "source": [
    "### Build time-based dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0ba4159",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_timed_data(df, df_transactions):\n",
    "    ZERO_ADDRESS = '0x0000000000000000000000000000000000000000'\n",
    "    column_names = [\n",
    "        \"date\", \n",
    "        \"days_since_mint\", \n",
    "        \"from_address\", \n",
    "        \"to_address\", \n",
    "        \"token_id\", \n",
    "        \"blk_number\", \n",
    "        \"eth_value\",\n",
    "        #\"usd_value\",\n",
    "        \"from_value\", \n",
    "        \"to_value\",\n",
    "    ]\n",
    "    \n",
    "    df_time = pd.DataFrame(columns=column_names)\n",
    "    df_total = df.shape[0]\n",
    "    \n",
    "    for index, row in tqdm(df.iterrows(), total=df_total):\n",
    "        blk_timestamp = row['blk_timestamp']\n",
    "        date = arrow.get(blk_timestamp).datetime\n",
    "\n",
    "        from_address = row['from_address']\n",
    "        to_address = row['to_address']\n",
    "        token_id = row['token_id']\n",
    "        blk_number = row['blk_number']\n",
    "        eth_value = row['eth_value']\n",
    "        \n",
    "        if from_address == ZERO_ADDRESS:\n",
    "            days_since_mint = 0\n",
    "            mint_date = date\n",
    "        else:\n",
    "            days_since_mint = date - mint_date\n",
    "            \n",
    "        #usd_value = get_usd_value(date, eth_value)\n",
    "            \n",
    "        from_value = lookup_transaction_value(df_transactions, blk_number, from_address)\n",
    "        to_value = lookup_transaction_value(df_transactions, blk_number, to_address)\n",
    "            \n",
    "        df_time = df_time.append({\n",
    "            'date': date,\n",
    "            'days_since_mint': days_since_mint,\n",
    "            'from_address': from_address,\n",
    "            'to_address': to_address,\n",
    "            'token_id': token_id, \n",
    "            'blk_number': blk_number,\n",
    "            'eth_value': eth_value,\n",
    "            #'usd_value': usd_value,\n",
    "            'from_value': from_value,\n",
    "            'to_value': to_value,\n",
    "        }, ignore_index=True)\n",
    "\n",
    "    return df_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c5ece0",
   "metadata": {},
   "source": [
    "### Build graph objects from time base dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04a3c719",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_graph_from_timed(df_time):    \n",
    "    # Building a network per block\n",
    "    # we will use a weighted and directed graph.\n",
    "    graph = nx.MultiDiGraph()\n",
    "\n",
    "    # loop over the pandas dataframe.\n",
    "    for index, row in tqdm(df_time.iterrows(), total=df_time.shape[0]):\n",
    "        # read the values from the dataframe.\n",
    "        # token_id  blk_timestamp eth_value \n",
    "        date = row['date']\n",
    "        from_address = row['from_address']\n",
    "        to_address = row['to_address']\n",
    "        token_id = row['token_id']\n",
    "        blk_number = row['blk_number']\n",
    "        eth_value = row['eth_value']\n",
    "        #usd_value = row['usd_value']\n",
    "        from_value = row['from_value']\n",
    "        to_value = row['to_value']\n",
    "        \n",
    "        # make sure both addresses are in the graph.\n",
    "        if from_address not in graph:\n",
    "            graph.add_node(from_address)\n",
    "        if to_address not in graph:\n",
    "            graph.add_node(to_address)\n",
    "\n",
    "        # set the attributes on this node.\n",
    "        nx.set_node_attributes(graph, {from_address: from_value, to_address: to_value}, 'eth_value')\n",
    "\n",
    "        # keep track of how many trades a wallet has done.\n",
    "        trades = nx.get_node_attributes(graph, \"trades\")\n",
    "        if from_address in trades:\n",
    "            nx.set_node_attributes(graph, {from_address:trades[from_address] + 1}, 'trades')\n",
    "        else:\n",
    "            nx.set_node_attributes(graph, {from_address:1}, 'trades')\n",
    "        if to_address in trades:\n",
    "            nx.set_node_attributes(graph, {to_address:trades[to_address] + 1}, 'trades')\n",
    "        else:\n",
    "            nx.set_node_attributes(graph, {to_address:1}, 'trades')\n",
    "\n",
    "        # check if this NFT has already been sold and if yes, remove the old sale.\n",
    "        # this might be a candidate for memoization - c.b.\n",
    "        remove_edges = []\n",
    "        for (u,v,d) in graph.edges.data():\n",
    "            if d['token_id'] == token_id:\n",
    "                remove_edges.append((u,v))\n",
    "        # we need to remove them in a seperate step, since otherwise we change the datastructure that we are iterating over.\n",
    "        for (u,v) in remove_edges:\n",
    "            graph.remove_edge(u,v)\n",
    "\n",
    "        # add an edge for the transaction.\n",
    "        graph.add_edge(from_address, to_address, weight=eth_value, token_id=token_id) # keep track of token id by adding it to the edge.\n",
    "        \n",
    "    return graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41bb9375",
   "metadata": {},
   "source": [
    "### Build time-based snapshots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15e3309b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_graph_snapshots(df_time):\n",
    "    \"\"\"Simpler version of below function which just returns the graph objects\"\"\"\n",
    "    res = {}\n",
    "    \n",
    "    df_time['date_quantile'], bins = pd.qcut(df_time['date'], 10, labels=False, retbins=True)\n",
    "    time_buckets = np.unique(df_time[\"date_quantile\"].to_numpy())\n",
    "    \n",
    "    for time_bucket, label in zip(time_buckets, bins):\n",
    "        selection = df_time[(df_time['date_quantile'] == time_bucket)]\n",
    "        graph_snapshot = build_graph_from_timed(selection)\n",
    "        res[label] = graph_snapshot\n",
    "        \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "38d51e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_df_snapshots(df_time):\n",
    "    column_names = [\n",
    "        \"time_bucket\", \n",
    "        \"time_bucket_label\",\n",
    "        \"number_of_nodes\", \n",
    "        \"avg_clustering\", \n",
    "        \"reciprocity\", \n",
    "        \"assortativity\", \n",
    "        \"assortativity_base\", \n",
    "        \"assortativity_out_out\", \n",
    "        \"assortativity_in_in\", \n",
    "        \"assortativity_in_out\",\n",
    "        \"centrality_degree\",\n",
    "        \"centrality_closeness\", \n",
    "        \"centrality_betweenness\",\n",
    "        \"centrality_eigenvector\",\n",
    "        \"avg_clustering_random\",\n",
    "        \"assortativity_random\"\n",
    "    ]\n",
    "    \n",
    "    df_snapshots = pd.DataFrame(columns=column_names)\n",
    "    \n",
    "    df_time['date_quantile'], bins = pd.qcut(df_time['date'], 10, labels=False, retbins=True)\n",
    "    time_buckets = np.unique(df_time[\"date_quantile\"].to_numpy())\n",
    "    \n",
    "    for time_bucket, label in zip(time_buckets, bins):\n",
    "        selection = df_time[(df_time['date_quantile'] == time_bucket)]\n",
    "        graph_snapshot = build_graph_from_timed(selection)\n",
    "        \n",
    "        df_snapshots = df_snapshots.append({\n",
    "            \"time_bucket\": time_bucket,\n",
    "            \"time_bucket_label\": label,\n",
    "            \"number_of_nodes\": graph_snapshot.number_of_nodes(),\n",
    "            \"reciprocity\": nx.reciprocity(graph_snapshot),\n",
    "            \"assortativity\": nx.degree_assortativity_coefficient(graph_snapshot),\n",
    "            \"assortativity_base\": nx.degree_pearson_correlation_coefficient(graph_snapshot.to_undirected(), weight='weight'),\n",
    "            \"assortativity_out_out\": nx.degree_pearson_correlation_coefficient(graph_snapshot, x='out', y='out', weight='weight'),\n",
    "            \"assortativity_in_in\": nx.degree_pearson_correlation_coefficient(graph_snapshot, x='in', y='in', weight='weight'),\n",
    "            \"assortativity_in_out\": nx.degree_pearson_correlation_coefficient(graph_snapshot, x='in', y='out', weight='weight'),\n",
    "            \"centrality_degree\": nx.degree_centrality(graph_snapshot),\n",
    "            \"centrality_closeness\": nx.closeness_centrality(graph_snapshot),\n",
    "        }, ignore_index=True)\n",
    "        \n",
    "    return df_snapshots.sort_values(by=['time_bucket'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d94a32b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 29120/29120 [05:09<00:00, 94.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 29120/29120 [01:43<00:00, 280.54it/s]\n",
      "100%|████████████████████████████████████| 15189/15189 [01:39<00:00, 153.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 15189/15189 [00:23<00:00, 653.64it/s]\n"
     ]
    }
   ],
   "source": [
    "for project in projects:\n",
    "    df_transactions = get_transaction_data(project)\n",
    "    df_time = create_timed_data(create_base_data(project), df_transactions)\n",
    "    print(len(errors))\n",
    "    \n",
    "    g_time = build_graph_from_timed(df_time)\n",
    "    \n",
    "    np.save(f\"./memory/{project}/full.npy\", df_time)\n",
    "    nx.write_gml(g_time, f\"./memory/{project}/full.gml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "10156e77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 3114/3114 [00:01<00:00, 2253.89it/s]\n",
      "100%|█████████████████████████████████████| 2779/2779 [00:01<00:00, 2477.83it/s]\n",
      "100%|█████████████████████████████████████| 2845/2845 [00:01<00:00, 1723.96it/s]\n",
      "100%|█████████████████████████████████████| 2910/2910 [00:02<00:00, 1091.22it/s]\n",
      "100%|██████████████████████████████████████| 2912/2912 [00:03<00:00, 955.94it/s]\n",
      "100%|█████████████████████████████████████| 2912/2912 [00:02<00:00, 1074.89it/s]\n",
      "100%|█████████████████████████████████████| 2913/2913 [00:02<00:00, 1162.21it/s]\n",
      "100%|█████████████████████████████████████| 2911/2911 [00:02<00:00, 1075.11it/s]\n",
      "100%|█████████████████████████████████████| 2913/2913 [00:02<00:00, 1174.51it/s]\n",
      "100%|█████████████████████████████████████| 2911/2911 [00:02<00:00, 1076.63it/s]\n",
      "100%|█████████████████████████████████████| 3114/3114 [00:01<00:00, 2174.75it/s]\n",
      "/Users/corey/uzh/HS21/network/venv/lib/python3.8/site-packages/scipy/stats/stats.py:4023: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n",
      "100%|█████████████████████████████████████| 2779/2779 [00:01<00:00, 2473.45it/s]\n",
      "/Users/corey/uzh/HS21/network/venv/lib/python3.8/site-packages/networkx/algorithms/assortativity/correlation.py:282: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return (xy * (M - ab)).sum() / np.sqrt(vara * varb)\n",
      "/Users/corey/uzh/HS21/network/venv/lib/python3.8/site-packages/scipy/stats/stats.py:4023: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n",
      "100%|█████████████████████████████████████| 2845/2845 [00:01<00:00, 1720.95it/s]\n",
      "100%|█████████████████████████████████████| 2910/2910 [00:02<00:00, 1105.73it/s]\n",
      "100%|██████████████████████████████████████| 2912/2912 [00:03<00:00, 957.90it/s]\n",
      "100%|█████████████████████████████████████| 2912/2912 [00:02<00:00, 1077.07it/s]\n",
      "100%|█████████████████████████████████████| 2913/2913 [00:02<00:00, 1164.06it/s]\n",
      "100%|█████████████████████████████████████| 2911/2911 [00:02<00:00, 1076.30it/s]\n",
      "100%|█████████████████████████████████████| 2913/2913 [00:02<00:00, 1149.82it/s]\n",
      "100%|█████████████████████████████████████| 2911/2911 [00:02<00:00, 1074.06it/s]\n",
      "100%|█████████████████████████████████████| 1519/1519 [00:00<00:00, 4158.10it/s]\n",
      "100%|█████████████████████████████████████| 1519/1519 [00:00<00:00, 3960.27it/s]\n",
      "100%|█████████████████████████████████████| 1519/1519 [00:00<00:00, 3584.68it/s]\n",
      "100%|█████████████████████████████████████| 1519/1519 [00:00<00:00, 3106.85it/s]\n",
      "100%|█████████████████████████████████████| 1519/1519 [00:00<00:00, 2568.56it/s]\n",
      "100%|█████████████████████████████████████| 1518/1518 [00:00<00:00, 2230.51it/s]\n",
      "100%|█████████████████████████████████████| 1519/1519 [00:00<00:00, 2193.94it/s]\n",
      "100%|█████████████████████████████████████| 1519/1519 [00:00<00:00, 2214.03it/s]\n",
      "100%|█████████████████████████████████████| 1519/1519 [00:00<00:00, 2340.52it/s]\n",
      "100%|█████████████████████████████████████| 1519/1519 [00:00<00:00, 2355.49it/s]\n",
      "100%|█████████████████████████████████████| 1519/1519 [00:00<00:00, 4155.08it/s]\n",
      "100%|█████████████████████████████████████| 1519/1519 [00:00<00:00, 3971.73it/s]\n",
      "100%|█████████████████████████████████████| 1519/1519 [00:00<00:00, 3587.31it/s]\n",
      "100%|█████████████████████████████████████| 1519/1519 [00:00<00:00, 3119.76it/s]\n",
      "100%|█████████████████████████████████████| 1519/1519 [00:00<00:00, 2569.66it/s]\n",
      "100%|█████████████████████████████████████| 1518/1518 [00:00<00:00, 2232.20it/s]\n",
      "100%|█████████████████████████████████████| 1519/1519 [00:00<00:00, 2342.08it/s]\n",
      "100%|█████████████████████████████████████| 1519/1519 [00:00<00:00, 2221.88it/s]\n",
      "100%|█████████████████████████████████████| 1519/1519 [00:00<00:00, 2339.09it/s]\n",
      "100%|█████████████████████████████████████| 1519/1519 [00:00<00:00, 2362.82it/s]\n"
     ]
    }
   ],
   "source": [
    "for project in projects:\n",
    "    column_names = [\n",
    "        \"date\", \n",
    "        \"days_since_mint\", \n",
    "        \"from_address\", \n",
    "        \"to_address\", \n",
    "        \"token_id\", \n",
    "        \"blk_number\", \n",
    "        \"eth_value\",\n",
    "        \"from_value\", \n",
    "        \"to_value\",\n",
    "    ]\n",
    "    \n",
    "    np_data = np.load(f\"./memory/{project}/full.npy\", allow_pickle=True)\n",
    "    df_time = pd.DataFrame(data=np_data, columns=column_names)\n",
    "    \n",
    "    g_snapshots = build_graph_snapshots(df_time)\n",
    "    df_snapshot_summary = build_df_snapshots(df_time)\n",
    "    \n",
    "    for i, snapshot in enumerate(g_snapshots.keys()):\n",
    "        nx.write_gml(g_time, f\"./memory/{project}/snapshots/{i}.gml\")\n",
    "    \n",
    "    np.save(f\"./memory/{project}/snapshots/summary.npy\", df_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e7715d0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         row     block        date  \\\n",
      "0          0  12253142  2021-04-16   \n",
      "1          1     block        date   \n",
      "2          2  12253527  2021-04-16   \n",
      "3          3  12253570  2021-04-16   \n",
      "4          4  12253684  2021-04-16   \n",
      "...      ...       ...         ...   \n",
      "21230  21230  13711959  2021-11-30   \n",
      "21231  21231  13711968  2021-11-30   \n",
      "21232  21232  13711968  2021-11-30   \n",
      "21233  21233  13711969  2021-11-30   \n",
      "21234  21234  13711969  2021-11-30   \n",
      "\n",
      "                                          address             eth_value  \\\n",
      "0      0x000000048797808be86aa9786c1f402671192d6b             1.3410686   \n",
      "1                                         address             eth_value   \n",
      "2      0x721931508df2764fd4f70c53da646cb8aed16ace  1.231442803923159847   \n",
      "3      0x721931508df2764fd4f70c53da646cb8aed16ace  1.193960158923159847   \n",
      "4      0xfa6c54de608c9a0a2c2a3220bb7e42b95d1b910b  0.473936397986028232   \n",
      "...                                           ...                   ...   \n",
      "21230  0xa145d810852499f0a45ae65d8545e74a78e9b92e  7.390824703182744369   \n",
      "21231  0xa145d810852499f0a45ae65d8545e74a78e9b92e  7.364025231836315154   \n",
      "21232  0xb14b87790643d2dab44b06692d37dd95b4b30e56                     0   \n",
      "21233  0xb14b87790643d2dab44b06692d37dd95b4b30e56                     0   \n",
      "21234  0xa145d810852499f0a45ae65d8545e74a78e9b92e  7.337746491885831224   \n",
      "\n",
      "                usd_value  \n",
      "0      3375.5098982580002  \n",
      "1               usd_value  \n",
      "2       3099.578480758711  \n",
      "3      3005.2335388143615  \n",
      "4      1192.9121318227726  \n",
      "...                   ...  \n",
      "21230  32884.883250835366  \n",
      "21231   32765.64114703714  \n",
      "21232                 0.0  \n",
      "21233                 0.0  \n",
      "21234  32648.715995926654  \n",
      "\n",
      "[21235 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "print(get_transaction_data('cyberkongz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a2602c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
